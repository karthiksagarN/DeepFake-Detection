{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
       "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
       "    You're <span style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; color: #15803d; font-weight: bold;\">connected</span> to Modelbit as karthik.sagarn@gmail.com.\n",
       "    Workspace: <span style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: bold; color: #845B99;\">karthiksagar</span>.\n",
       "    \n",
       "      Region: <span style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: bold; color: #845B99;\">us-east-1</span>\n",
       "    \n",
       "    Branch: <span style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: bold; color: #845B99;\">main</span>\n",
       "\t</div>\n",
       "  \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import modelbit\n",
    "mb = modelbit.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL TIME MODEL DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes=2, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n",
    "        super(Model, self).__init__()\n",
    "        model = models.resnext50_32x4d(pretrained=True)  # Residual Network CNN\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(2048, num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.dp(self.linear1(x))\n",
    "\n",
    "def detection(frame_base64):\n",
    "    frame_data = base64.b64decode(frame_base64)\n",
    "    frame = Image.open(BytesIO(frame_data))\n",
    "\n",
    "    im_size = 224\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((im_size, im_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    frame = transform(frame).unsqueeze(0)\n",
    "    \n",
    "    # Load the TorchScript model\n",
    "    model_path = \"traced_model.pt\"\n",
    "    model = torch.jit.load(model_path)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        frame_tensor = frame\n",
    "        logits = model(frame_tensor)\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        probabilities = sm(logits)\n",
    "        confidence, prediction = torch.max(probabilities, dim=1)    \n",
    "\n",
    "    if prediction == 1:\n",
    "        result = {\n",
    "            \"status\": \"real\",\n",
    "            \"confidence\": confidence.item() * 100\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            \"status\": \"fake\",\n",
    "            \"confidence\": confidence.item() * 100\n",
    "        }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
       "  <div>\n",
       "    <span style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #15803d;\">Deploying </span> <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">detection</span>\n",
       "  </div>\n",
       "  \n",
       "\n",
       "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">Uploading dependencies...</div>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
       "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #15803d;\">Success!</div>\n",
       "  \n",
       "    <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
       "      Deployment <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">detection</span>\n",
       "      will be ready in  a few seconds!\n",
       "    </div>\n",
       "  \n",
       "\n",
       "  <a href=\"https://us-east-1.modelbit.com/w/karthiksagar/main/deployments/detection/apis\" target=\"_blank\" style=\"display: inline-block; margin-top: 12px;\" >\n",
       "    <div\n",
       "      style=\"display: inline-block; background-color: #845B99; border-radius: 0.375rem; color: white; cursor: pointer; font-size: 14px; font-weight: 700; padding: 8px 16px;\"\n",
       "      onmouseenter=\"this.style.background='#714488'\"\n",
       "      onmouseleave=\"this.style.background='#845B99'\"\n",
       "    >\n",
       "      View in Modelbit\n",
       "    </div>\n",
       "  </a>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mb.deploy(detection, extra_files={\"/Users/karthiksagar/DeepFake-Detection/saved_best_model/traced_model.pt\" : \"traced_model.pt\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPLOAD VIDEO DEPLOYMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without base64 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Model class\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes=2, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n",
    "        super(Model, self).__init__()\n",
    "        model = models.resnext50_32x4d(pretrained=True)  # Residual Network CNN\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(2048, num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.dp(self.linear1(x))\n",
    "\n",
    "# Preprocessing transforms (same as during training)\n",
    "im_size = 224\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((im_size, im_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "def extract_frames(video_path, frame_count=20):\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_interval = total_frames // frame_count\n",
    "    \n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "    \n",
    "    while success and len(frames) < frame_count:\n",
    "        if count % frame_interval == 0:\n",
    "            # Convert to PIL Image and append\n",
    "            frames.append(Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)))\n",
    "        success, image = vidcap.read()\n",
    "        count += 1\n",
    "    \n",
    "    vidcap.release()\n",
    "    return frames\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "\n",
    "    preprocessed_frames = [transform(frame) for frame in frames]\n",
    "    return torch.stack(preprocessed_frames).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "def predict_video(video_path):\n",
    "\n",
    "    # Load the model\n",
    "    model_path = 'traced_model.pt'\n",
    "    model = torch.jit.load(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract and preprocess frames from the video\n",
    "    frames = extract_frames(video_path, frame_count=20)\n",
    "    frame_tensor = preprocess_frames(frames)\n",
    "    \n",
    "    # Make prediction on the frame sequence\n",
    "    with torch.no_grad():\n",
    "        logits = model(frame_tensor)\n",
    "        sm = torch.nn.Softmax(dim=1)\n",
    "        probabilities = sm(logits)\n",
    "        confidence, prediction = torch.max(probabilities, dim=1)\n",
    "    \n",
    "    result = {\n",
    "        \"status\": \"real\" if prediction.item() == 1 else \"fake\",\n",
    "        \"confidence\": confidence.item() * 100\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
       "  <div>\n",
       "    <span style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #15803d;\">Deploying </span> <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">predict_video</span>\n",
       "  </div>\n",
       "  \n",
       "\n",
       "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">Uploading dependencies...</div>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
       "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #15803d;\">Success!</div>\n",
       "  \n",
       "    <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
       "      Deployment <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">predict_video</span>\n",
       "      will be ready in  a few seconds!\n",
       "    </div>\n",
       "  \n",
       "\n",
       "  <a href=\"https://us-east-1.modelbit.com/w/karthiksagar/main/deployments/predict_video/apis\" target=\"_blank\" style=\"display: inline-block; margin-top: 12px;\" >\n",
       "    <div\n",
       "      style=\"display: inline-block; background-color: #845B99; border-radius: 0.375rem; color: white; cursor: pointer; font-size: 14px; font-weight: 700; padding: 8px 16px;\"\n",
       "      onmouseenter=\"this.style.background='#714488'\"\n",
       "      onmouseleave=\"this.style.background='#845B99'\"\n",
       "    >\n",
       "      View in Modelbit\n",
       "    </div>\n",
       "  </a>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mb.deploy(predict_video, extra_files={\"/Users/karthiksagar/DeepFake-Detection/saved_best_model/traced_model.pt\" : \"traced_model.pt\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with base64 encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import os\n",
    "\n",
    "def extract_faces_from_video(video_path, frame_count=20):\n",
    "    # Load face cascade dynamically\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    faces = []\n",
    "    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_interval = total_frames // frame_count\n",
    "    \n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "    \n",
    "    while success and len(faces) < frame_count:\n",
    "        if count % frame_interval == 0:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            detected_faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "            for (x, y, w, h) in detected_faces:\n",
    "                face = image[y:y+h, x:x+w]\n",
    "                if face.size > 0:\n",
    "                    face_pil = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
    "                    faces.append(face_pil)\n",
    "        success, image = vidcap.read()\n",
    "        count += 1\n",
    "    \n",
    "    vidcap.release()\n",
    "    return faces\n",
    "\n",
    "def preprocess_faces(faces):\n",
    "    # Preprocessing transforms (same as during training)\n",
    "    im_size = 224\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((im_size, im_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    preprocessed_faces = [transform(face) for face in faces]\n",
    "    return torch.stack(preprocessed_faces).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "def decode_base64_video(video_base64):\n",
    "    # Decode the base64 video to binary data\n",
    "    video_data = base64.b64decode(video_base64)\n",
    "    \n",
    "    # Save the binary data as a temporary video file\n",
    "    video_path = 'temp_video.mp4'\n",
    "    with open(video_path, 'wb') as f:\n",
    "        f.write(video_data)\n",
    "    \n",
    "    return video_path\n",
    "\n",
    "def predict_video_base64(video_base64):\n",
    "    # Decode the base64 video\n",
    "    video_path = decode_base64_video(video_base64)\n",
    "\n",
    "    # Load the model\n",
    "    model_path = 'traced_model.pt'\n",
    "    model = torch.jit.load(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract faces from the video\n",
    "    faces = extract_faces_from_video(video_path, frame_count=20)\n",
    "    if not faces:\n",
    "        # Handle case where no faces were detected\n",
    "        return {\"status\": \"error\", \"message\": \"No faces detected\"}\n",
    "    \n",
    "    # Preprocess faces\n",
    "    face_tensor = preprocess_faces(faces)\n",
    "    \n",
    "    # Make prediction on the face sequence\n",
    "    with torch.no_grad():\n",
    "        logits = model(face_tensor)\n",
    "        sm = torch.nn.Softmax(dim=1)\n",
    "        probabilities = sm(logits)\n",
    "        confidence, prediction = torch.max(probabilities, dim=1)\n",
    "    \n",
    "    if prediction == 1:\n",
    "        result = {\n",
    "            \"status\": \"real\",\n",
    "            \"confidence\": confidence.item() * 100\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            \"status\": \"fake\",\n",
    "            \"confidence\": confidence.item() * 100\n",
    "        }\n",
    "    \n",
    "    # Remove the temporary video file after processing\n",
    "    os.remove(video_path)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
       "  <div>\n",
       "    <span style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #15803d;\">Deploying </span> <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">predict_video_base64</span>\n",
       "  </div>\n",
       "  \n",
       "\n",
       "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">Uploading dependencies...</div>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
       "  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #15803d;\">Success!</div>\n",
       "  \n",
       "    <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n",
       "      Deployment <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">predict_video_base64</span>\n",
       "      will be ready in  a few seconds!\n",
       "    </div>\n",
       "  \n",
       "\n",
       "  <a href=\"https://us-east-1.modelbit.com/w/karthiksagar/main/deployments/predict_video_base64/apis\" target=\"_blank\" style=\"display: inline-block; margin-top: 12px;\" >\n",
       "    <div\n",
       "      style=\"display: inline-block; background-color: #845B99; border-radius: 0.375rem; color: white; cursor: pointer; font-size: 14px; font-weight: 700; padding: 8px 16px;\"\n",
       "      onmouseenter=\"this.style.background='#714488'\"\n",
       "      onmouseleave=\"this.style.background='#845B99'\"\n",
       "    >\n",
       "      View in Modelbit\n",
       "    </div>\n",
       "  </a>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mb.deploy(predict_video_base64, extra_files={\"/Users/karthiksagar/DeepFake-Detection/saved_best_model/traced_model.pt\" : \"traced_model.pt\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch to Jit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes=2, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n",
    "        super(Model, self).__init__()\n",
    "        model = models.resnext50_32x4d(pretrained=True)  # Residual Network CNN\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional, batch_first=True)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should have shape [batch_size, seq_len, channels, height, width]\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "\n",
    "        # Process each frame independently through the CNN\n",
    "        x = x.view(-1, c, h, w)  # Reshape to [batch_size * seq_len, channels, height, width]\n",
    "        x = self.model(x)  # Apply CNN\n",
    "        x = self.avgpool(x)  # Apply pooling\n",
    "        x = x.view(batch_size, seq_len, -1)  # Reshape to [batch_size, seq_len, latent_dim]\n",
    "\n",
    "        # Apply LSTM\n",
    "        x, _ = self.lstm(x)  # Get the output and hidden states\n",
    "        x = x[:, -1]  # Take the output from the last time step\n",
    "        x = self.relu(x)  # Apply activation function\n",
    "        x = self.dp(x)  # Apply dropout\n",
    "        x = self.linear1(x)  # Apply the final linear layer\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes=2, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n",
    "        super(Model, self).__init__()\n",
    "        model = models.resnext50_32x4d(pretrained=True)  # Residual Network CNN\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional, batch_first=True)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should have shape [batch_size, seq_len, channels, height, width]\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        \n",
    "        # Process each frame independently\n",
    "        x = x.view(-1, c, h, w)  # Reshape to [batch_size * seq_len, channels, height, width]\n",
    "        x = self.model(x)  # Apply CNN\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(batch_size, seq_len, -1)  # Reshape to [batch_size, seq_len, latent_dim]\n",
    "        \n",
    "        # Apply LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Process the LSTM output\n",
    "        x = self.relu(x[:, -1])  # Get the output of the last time step\n",
    "        x = self.dp(self.linear1(x))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(2)\n",
    "ff_traced_model = torch.jit.script(model)\n",
    "ff_traced_model.save('/Users/karthiksagar/DeepFake-Detection/saved_best_model/traced_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_video_base64(video_base64):\n",
    "#     # Decode the base64 video\n",
    "#     video_path = decode_base64_video(video_base64)\n",
    "\n",
    "#     # Load the model\n",
    "#     model_path = 'traced_model.pt'\n",
    "#     model = torch.jit.load(model_path)\n",
    "#     model.eval()\n",
    "\n",
    "#     # Extract and preprocess frames from the video\n",
    "#     frames = extract_frames(video_path, frame_count=60)\n",
    "\n",
    "#     highest_confidence_result = None\n",
    "#     highest_confidence = 0  # Initialize the highest confidence to 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for frame in frames:\n",
    "#             frame_tensor = transform(frame).unsqueeze(0)  # Transform and add batch dimension\n",
    "#             logits = model(frame_tensor)\n",
    "#             sm = torch.nn.Softmax(dim=1)\n",
    "#             probabilities = sm(logits)\n",
    "#             confidence, prediction = torch.max(probabilities, dim=1)\n",
    "\n",
    "#             # Create the result dictionary for the current frame\n",
    "#             result = {\n",
    "#                 \"status\": \"real\" if prediction.item() == 1 else \"fake\",\n",
    "#                 \"confidence\": confidence.item() * 100\n",
    "#             }\n",
    "\n",
    "#             # Update the highest confidence result if the curr\n",
    "# ent confidence is higher\n",
    "#             if confidence.item() > highest_confidence:\n",
    "#                 highest_confidence = confidence.item()\n",
    "#                 highest_confidence_result = result\n",
    "\n",
    "#     return highest_confidence_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
